{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fce9b84d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b04129",
   "metadata": {},
   "source": [
    "## Exporting Examples in langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86f12cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# QA\n",
    "\n",
    "inputs = [\n",
    "    \"What problem does MemR3 aim to solve in LLM memory systems?\",\n",
    "    \"What are the two core mechanisms of MemR3?\",\n",
    "    \"How does MemR3 differ from the standard retrieve-then-answer pipeline?\",\n",
    "    \"What is the role of the evidence–gap tracker in MemR3?\",\n",
    "    \"What actions can the MemR3 router choose from?\",\n",
    "    \"Why is MemR3 considered plug-and-play?\",\n",
    "    \"On which benchmark was MemR3 evaluated?\",\n",
    "    \"What metrics were used to evaluate MemR3?\",\n",
    "    \"How much does MemR3 improve RAG and Zep on GPT-4.1-mini?\",\n",
    "    \"Which question types benefit the most from MemR3?\",\n",
    "    \"Why does Full-Context sometimes perform worse than MemR3?\",\n",
    "    \"What backend memory systems were used with MemR3?\",\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    \"MemR3 addresses the lack of closed-loop, explicit control in memory retrieval for LLM agents, reducing noisy, inefficient, and incomplete retrieval.\",\n",
    "    \"MemR3 uses a router that selects retrieve, reflect, or answer actions, and a global evidence–gap tracker that tracks known evidence and missing information.\",\n",
    "    \"Instead of a single retrieval pass, MemR3 uses a closed-loop process with iterative retrieval, reflection, and early stopping based on evidence completeness.\",\n",
    "    \"The evidence–gap tracker explicitly records what has been established as evidence and what information is still missing to answer the query correctly.\",\n",
    "    \"The router can choose among retrieve, reflect, and answer actions at each iteration.\",\n",
    "    \"MemR3 acts as an external controller and can be integrated with any existing memory backend that returns text snippets, without changing storage architecture.\",\n",
    "    \"MemR3 was evaluated on the LoCoMo benchmark for long-term conversational memory.\",\n",
    "    \"Answer quality was measured using LLM-as-a-Judge scores, evaluated by GPT-4.1.\",\n",
    "    \"With GPT-4.1-mini, MemR3 improves RAG by +7.29% and Zep by +1.94% overall.\",\n",
    "    \"Temporal and multi-hop questions benefit the most from MemR3 due to explicit gap tracking and iterative retrieval.\",\n",
    "    \"Full-Context often overloads the LLM with irrelevant or noisy memories, which hurts reasoning, especially for temporal and open-domain queries.\",\n",
    "    \"MemR3 was tested with chunk-based RAG and graph-based Zep memory backends.\",\n",
    "]\n",
    "\n",
    "data = [{'question':q,'output':o} for q,o in zip(inputs,outputs)]\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.to_csv('/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/data/docs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27d2eb94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_ids': ['d330d1bb-d6aa-4f45-80a1-e555944e794f',\n",
       "  '94088886-bf08-44f8-85a7-798e99580545',\n",
       "  '8f0e89fd-88e9-413a-a298-1eeeaf26d401',\n",
       "  '27400071-3ad2-4173-912e-a3ff1d8adcfc',\n",
       "  '58d67563-fb1a-4a14-a322-85856a73a1ea',\n",
       "  '219de6ce-10d4-4357-aa4b-32157e919549',\n",
       "  '944bf6fc-cbd6-4ce2-a640-aa65240d439e',\n",
       "  'a2eba90c-f210-4fed-86a8-50289d8e3edc',\n",
       "  'bf44a6db-4b96-4537-b507-9f73185741d0',\n",
       "  'a18e123b-7863-45b7-8ef0-487b58ce2942',\n",
       "  'ce24e6d2-99e2-4762-b8f3-5f46cac00419',\n",
       "  '81bcebff-dcfc-4a71-97ca-385fd8ff9804'],\n",
       " 'count': 12}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"AgenticAImemoryqa\"\n",
    "\n",
    "# Store\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Input and expected output pairs for AgenticAIResearch\",\n",
    ")\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in inputs],\n",
    "    outputs=[{\"answer\": a} for a in outputs],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a72eff7",
   "metadata": {},
   "source": [
    "# getting the rag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61ece96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project\")\n",
    "\n",
    "from pathlib import Path\n",
    "from multi_doc_chat.src.DataIngestion import ChatIngestor\n",
    "from multi_doc_chat.src.retreival import ConversationalRAG\n",
    "import os\n",
    "\n",
    "# Simple file adapter for local file paths\n",
    "class LocalFileAdapter:\n",
    "    \"\"\"Adapter for local file paths to work with ChatIngestor.\"\"\"\n",
    "    def __init__(self, file_path: str):\n",
    "        self.path = Path(file_path)\n",
    "        self.name = self.path.name\n",
    "    \n",
    "    def getbuffer(self) -> bytes:\n",
    "        return self.path.read_bytes()\n",
    "\n",
    "\n",
    "def answer_ai_report_question(\n",
    "    inputs: dict,\n",
    "    data_path: str = \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/data/doc.txt\",\n",
    "    chunk_size: int = 1000,\n",
    "    chunk_overlap: int = 200,\n",
    "    k: int = 5\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Answer questions about the AI Engineering Report using RAG.\n",
    "    \n",
    "    Args:\n",
    "        inputs: Dictionary containing the question, e.g., {\"question\": \"What is RAG?\"}\n",
    "        data_path: Path to the AI agent memory Report text file\n",
    "        chunk_size: Size of text chunks for splitting\n",
    "        chunk_overlap: Overlap between chunks\n",
    "        k: Number of documents to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with the answer, e.g., {\"answer\": \"RAG stands for...\"}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract question from inputs\n",
    "        question = inputs.get(\"question\", \"\")\n",
    "        if not question:\n",
    "            return {\"answer\": \"No question provided\"}\n",
    "        \n",
    "        # Check if file exists\n",
    "        if not Path(data_path).exists():\n",
    "            return {\"answer\": f\"Data file not found: {data_path}\"}\n",
    "        \n",
    "        # Create file adapter\n",
    "        file_adapter = LocalFileAdapter(data_path)\n",
    "        \n",
    "        # Build index using ChatIngestor\n",
    "        ingestor = ChatIngestor(\n",
    "            temp_base=\"data\",\n",
    "            faiss_base=\"faiss_index\",\n",
    "            use_session_dirs=True\n",
    "        )\n",
    "        \n",
    "        # Build retriever\n",
    "        ingestor.built_retriver(\n",
    "            uploaded_files=[file_adapter],\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            k=k\n",
    "        )\n",
    "        \n",
    "        # Get session ID and index path\n",
    "        session_id = ingestor.session_id\n",
    "        index_path = f\"faiss_index/{session_id}\"\n",
    "        \n",
    "        # Create RAG instance and load retriever\n",
    "        rag = ConversationalRAG(session_id=session_id)\n",
    "        rag.load_retriever_from_faiss(\n",
    "            index_path=index_path,\n",
    "            k=k,\n",
    "            index_name=os.getenv(\"FAISS_INDEX_NAME\", \"index\")\n",
    "        )\n",
    "        \n",
    "        # Get answer\n",
    "        answer = rag.invoke(question, chat_history=[])\n",
    "        \n",
    "        return {\"answer\": answer}\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"answer\": f\"Error: {str(e)}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "464fe170",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"timestamp\": \"2025-12-27T14:10:26.978215Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"session_id\": \"session_20251227_194026_99e5741d\", \"temp_dir\": \"data/session_20251227_194026_99e5741d\", \"faiss_dir\": \"faiss_index/session_20251227_194026_99e5741d\", \"sessionized\": true, \"timestamp\": \"2025-12-27T14:10:26.979899Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"original_filename\": \"doc.txt\", \"saved_as\": \"data/session_20251227_194026_99e5741d/doc_f7f62f14.txt\", \"timestamp\": \"2025-12-27T14:10:26.982979Z\", \"level\": \"info\", \"event\": \"File saved successfully\"}\n",
      "{\"count\": 1, \"timestamp\": \"2025-12-27T14:10:26.984315Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 82, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-12-27T14:10:26.988389Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "Loading faiss with AVX512 support.\n",
      "Successfully loaded faiss with AVX512 support.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "{\"added\": 1, \"index\": \"faiss_index/session_20251227_194026_99e5741d\", \"timestamp\": \"2025-12-27T14:11:28.047928Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2025-12-27T14:11:28.053952Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2025-12-27T14:11:28.104590Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"provider\": \"google\", \"model\": \"gemini-2.5-flash-lite\", \"timestamp\": \"2025-12-27T14:11:28.128063Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20251227_194026_99e5741d\", \"timestamp\": \"2025-12-27T14:11:28.585926Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20251227_194026_99e5741d\", \"timestamp\": \"2025-12-27T14:11:28.587366Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2025-12-27T14:11:28.593651Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"session_id\": \"session_20251227_194026_99e5741d\", \"timestamp\": \"2025-12-27T14:11:28.928583Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20251227_194026_99e5741d\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20251227_194026_99e5741d\", \"timestamp\": \"2025-12-27T14:11:28.933188Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "AFC is enabled with max remote calls: 10.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "AFC is enabled with max remote calls: 10.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n",
      "{\"session_id\": \"session_20251227_194026_99e5741d\", \"user_input\": \"What are the two core mechanisms of MemR3?\", \"answer_preview\": \"The two core mechanisms of MemR3 are the explicit maintenance and updating of evidence (E) and the gap (G), which summarize what the agent knows and s\", \"timestamp\": \"2025-12-27T14:11:32.123399Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the two core mechanisms of MemR3?\n",
      "\n",
      "Answer: The two core mechanisms of MemR3 are the explicit maintenance and updating of evidence (E) and the gap (G), which summarize what the agent knows and still needs to know. Additionally, MemR3 uses a router to choose actions like retrieve, reflect, or answer, transforming the process into a closed-loop controller. This allows it to refine retrieval queries, integrate new evidence, and stop early when the information gap is resolved.\n"
     ]
    }
   ],
   "source": [
    "# testing the rag\n",
    "test_input = {'question':'What are the two core mechanisms of MemR3?'}\n",
    "result = answer_ai_report_question(test_input)\n",
    "print(\"Question:\", test_input[\"question\"])\n",
    "print(\"\\nAnswer:\", result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555b7683",
   "metadata": {},
   "source": [
    "# Evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8820d021",
   "metadata": {},
   "source": [
    "## Builtin evaluator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2a3333f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-agenticAIReport-qa-rag-7194569d' at:\n",
      "https://smith.langchain.com/o/b6ae6d41-8930-4df1-adfd-5dea23b1b78b/datasets/c2205ae9-db43-4acb-b93f-8b2430046d72/compare?selectedSessions=96118258-6556-400e-898c-da432800fba6\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]{\"timestamp\": \"2025-12-27T15:46:31.072492Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"session_id\": \"session_20251227_211631_d380f1f8\", \"temp_dir\": \"data/session_20251227_211631_d380f1f8\", \"faiss_dir\": \"faiss_index/session_20251227_211631_d380f1f8\", \"sessionized\": true, \"timestamp\": \"2025-12-27T15:46:31.081161Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"original_filename\": \"doc.txt\", \"saved_as\": \"data/session_20251227_211631_d380f1f8/doc_2d249a07.txt\", \"timestamp\": \"2025-12-27T15:46:31.084725Z\", \"level\": \"info\", \"event\": \"File saved successfully\"}\n",
      "{\"count\": 1, \"timestamp\": \"2025-12-27T15:46:31.086990Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 82, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-12-27T15:46:31.100132Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "Loading faiss with AVX512 support.\n",
      "Successfully loaded faiss with AVX512 support.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "{\"added\": 1, \"index\": \"faiss_index/session_20251227_211631_d380f1f8\", \"timestamp\": \"2025-12-27T15:46:34.498042Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2025-12-27T15:46:34.501002Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2025-12-27T15:46:34.510342Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"provider\": \"google\", \"model\": \"gemini-2.5-flash-lite\", \"timestamp\": \"2025-12-27T15:46:34.514867Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20251227_211631_d380f1f8\", \"timestamp\": \"2025-12-27T15:46:34.703888Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20251227_211631_d380f1f8\", \"timestamp\": \"2025-12-27T15:46:34.704517Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2025-12-27T15:46:34.707768Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"session_id\": \"session_20251227_211631_d380f1f8\", \"timestamp\": \"2025-12-27T15:46:34.937607Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20251227_211631_d380f1f8\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20251227_211631_d380f1f8\", \"timestamp\": \"2025-12-27T15:46:34.941353Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "AFC is enabled with max remote calls: 10.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "AFC is enabled with max remote calls: 10.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n",
      "{\"session_id\": \"session_20251227_211631_d380f1f8\", \"user_input\": \"What backend memory systems were used with MemR3?\", \"answer_preview\": \"MemR3 was used with RAG and Zep as retriever backends. Vanilla RAG and Zep are described as existing memory systems that can be plugged into MemR3.\", \"timestamp\": \"2025-12-27T15:46:38.098706Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n",
      "NumExpr defaulting to 4 threads.\n",
      "Error running evaluator <DynamicRunEvaluator _wrapped_evaluator> on run 019b607d-6c11-7cf2-8e24-56602bb17edb: GoogleAuthError(\"Unable to find your project. Please provide a project ID by:\\n- Passing a constructor argument\\n- Using vertexai.init()\\n- Setting project using 'gcloud config set project my-project'\\n- Setting a GCP environment variable\\n- To create a Google Cloud project, please follow guidance at https://developers.google.com/workspace/guides/create-project\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/initializer.py\", line 367, in project\n",
      "    self._set_project_as_env_var_or_google_auth_default()\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/initializer.py\", line 114, in _set_project_as_env_var_or_google_auth_default\n",
      "    credentials, project = google.auth.default()\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/auth/_default.py\", line 739, in default\n",
      "    raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1585, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 353, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 779, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/llm.py\", line 566, in _wrapped_evaluator\n",
      "    res = _run_evaluator_untyped(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/utils.py\", line 219, in _run_evaluator_untyped\n",
      "    results = _run_scorer(**kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/utils.py\", line 132, in _run_scorer\n",
      "    score = scorer(**kwargs)\n",
      "            ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/llm.py\", line 238, in get_score\n",
      "    judge = init_chat_model(model=model)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain/chat_models/base.py\", line 316, in init_chat_model\n",
      "    return _init_chat_model_helper(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain/chat_models/base.py\", line 368, in _init_chat_model_helper\n",
      "    return ChatVertexAI(model=model, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 239, in warn_if_direct_instance\n",
      "    return wrapped(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_google_vertexai/chat_models.py\", line 1831, in __init__\n",
      "    super().__init__(**kwargs)\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_core/load/serializable.py\", line 116, in __init__\n",
      "    super().__init__(*args, **kwargs)\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/pydantic/main.py\", line 250, in __init__\n",
      "    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_google_vertexai/_base.py\", line 156, in validate_project\n",
      "    self.project = initializer.global_config.project\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/initializer.py\", line 370, in project\n",
      "    raise GoogleAuthError(project_not_found_exception_str) from exc\n",
      "google.auth.exceptions.GoogleAuthError: Unable to find your project. Please provide a project ID by:\n",
      "- Passing a constructor argument\n",
      "- Using vertexai.init()\n",
      "- Setting project using 'gcloud config set project my-project'\n",
      "- Setting a GCP environment variable\n",
      "- To create a Google Cloud project, please follow guidance at https://developers.google.com/workspace/guides/create-project\n",
      "1it [00:31, 31.51s/it]{\"timestamp\": \"2025-12-27T15:47:02.575679Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"session_id\": \"session_20251227_211702_2f2d7877\", \"temp_dir\": \"data/session_20251227_211702_2f2d7877\", \"faiss_dir\": \"faiss_index/session_20251227_211702_2f2d7877\", \"sessionized\": true, \"timestamp\": \"2025-12-27T15:47:02.580053Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"original_filename\": \"doc.txt\", \"saved_as\": \"data/session_20251227_211702_2f2d7877/doc_c80b3b53.txt\", \"timestamp\": \"2025-12-27T15:47:02.581588Z\", \"level\": \"info\", \"event\": \"File saved successfully\"}\n",
      "{\"count\": 1, \"timestamp\": \"2025-12-27T15:47:02.583415Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 82, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-12-27T15:47:02.587601Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "{\"added\": 1, \"index\": \"faiss_index/session_20251227_211702_2f2d7877\", \"timestamp\": \"2025-12-27T15:47:06.678648Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2025-12-27T15:47:06.679643Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2025-12-27T15:47:06.684620Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"provider\": \"google\", \"model\": \"gemini-2.5-flash-lite\", \"timestamp\": \"2025-12-27T15:47:06.685420Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20251227_211702_2f2d7877\", \"timestamp\": \"2025-12-27T15:47:06.884861Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20251227_211702_2f2d7877\", \"timestamp\": \"2025-12-27T15:47:06.887659Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2025-12-27T15:47:06.894219Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"session_id\": \"session_20251227_211702_2f2d7877\", \"timestamp\": \"2025-12-27T15:47:07.070530Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20251227_211702_2f2d7877\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20251227_211702_2f2d7877\", \"timestamp\": \"2025-12-27T15:47:07.071419Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "AFC is enabled with max remote calls: 10.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "AFC is enabled with max remote calls: 10.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n",
      "{\"session_id\": \"session_20251227_211702_2f2d7877\", \"user_input\": \"How much does MemR3 improve RAG and Zep on GPT-4.1-mini?\", \"answer_preview\": \"MemR3 improves RAG from 73.05% to 81.20% and Zep from 72.34% to 77.78% on GPT-4.1-mini. These gains suggest that explicitly tracking evidence and gaps\", \"timestamp\": \"2025-12-27T15:47:12.337554Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n",
      "Error running evaluator <DynamicRunEvaluator _wrapped_evaluator> on run 019b607d-e72b-7982-a250-2642b6a5d99b: GoogleAuthError(\"Unable to find your project. Please provide a project ID by:\\n- Passing a constructor argument\\n- Using vertexai.init()\\n- Setting project using 'gcloud config set project my-project'\\n- Setting a GCP environment variable\\n- To create a Google Cloud project, please follow guidance at https://developers.google.com/workspace/guides/create-project\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/initializer.py\", line 367, in project\n",
      "    self._set_project_as_env_var_or_google_auth_default()\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/initializer.py\", line 114, in _set_project_as_env_var_or_google_auth_default\n",
      "    credentials, project = google.auth.default()\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/auth/_default.py\", line 739, in default\n",
      "    raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1585, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 353, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 779, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/llm.py\", line 566, in _wrapped_evaluator\n",
      "    res = _run_evaluator_untyped(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/utils.py\", line 219, in _run_evaluator_untyped\n",
      "    results = _run_scorer(**kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/utils.py\", line 132, in _run_scorer\n",
      "    score = scorer(**kwargs)\n",
      "            ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/llm.py\", line 238, in get_score\n",
      "    judge = init_chat_model(model=model)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain/chat_models/base.py\", line 316, in init_chat_model\n",
      "    return _init_chat_model_helper(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain/chat_models/base.py\", line 368, in _init_chat_model_helper\n",
      "    return ChatVertexAI(model=model, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 239, in warn_if_direct_instance\n",
      "    return wrapped(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_google_vertexai/chat_models.py\", line 1831, in __init__\n",
      "    super().__init__(**kwargs)\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_core/load/serializable.py\", line 116, in __init__\n",
      "    super().__init__(*args, **kwargs)\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/pydantic/main.py\", line 250, in __init__\n",
      "    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_google_vertexai/_base.py\", line 156, in validate_project\n",
      "    self.project = initializer.global_config.project\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/initializer.py\", line 370, in project\n",
      "    raise GoogleAuthError(project_not_found_exception_str) from exc\n",
      "google.auth.exceptions.GoogleAuthError: Unable to find your project. Please provide a project ID by:\n",
      "- Passing a constructor argument\n",
      "- Using vertexai.init()\n",
      "- Setting project using 'gcloud config set project my-project'\n",
      "- Setting a GCP environment variable\n",
      "- To create a Google Cloud project, please follow guidance at https://developers.google.com/workspace/guides/create-project\n",
      "2it [00:53, 25.95s/it]{\"timestamp\": \"2025-12-27T15:47:24.648730Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"session_id\": \"session_20251227_211724_119ac115\", \"temp_dir\": \"data/session_20251227_211724_119ac115\", \"faiss_dir\": \"faiss_index/session_20251227_211724_119ac115\", \"sessionized\": true, \"timestamp\": \"2025-12-27T15:47:24.651277Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"original_filename\": \"doc.txt\", \"saved_as\": \"data/session_20251227_211724_119ac115/doc_0cff48dd.txt\", \"timestamp\": \"2025-12-27T15:47:24.653182Z\", \"level\": \"info\", \"event\": \"File saved successfully\"}\n",
      "{\"count\": 1, \"timestamp\": \"2025-12-27T15:47:24.656054Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 82, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-12-27T15:47:24.672981Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "{\"added\": 1, \"index\": \"faiss_index/session_20251227_211724_119ac115\", \"timestamp\": \"2025-12-27T15:47:29.297422Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2025-12-27T15:47:29.306912Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2025-12-27T15:47:29.355681Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"provider\": \"google\", \"model\": \"gemini-2.5-flash-lite\", \"timestamp\": \"2025-12-27T15:47:29.366690Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20251227_211724_119ac115\", \"timestamp\": \"2025-12-27T15:47:29.595547Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20251227_211724_119ac115\", \"timestamp\": \"2025-12-27T15:47:29.597997Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2025-12-27T15:47:29.601836Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"session_id\": \"session_20251227_211724_119ac115\", \"timestamp\": \"2025-12-27T15:47:29.797623Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20251227_211724_119ac115\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20251227_211724_119ac115\", \"timestamp\": \"2025-12-27T15:47:29.798345Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "AFC is enabled with max remote calls: 10.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "AFC is enabled with max remote calls: 10.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n",
      "{\"session_id\": \"session_20251227_211724_119ac115\", \"user_input\": \"Why does Full-Context sometimes perform worse than MemR3?\", \"answer_preview\": \"Full-Context consumes significantly more tokens than MemR3, and it only surpasses MemR3 on multi-hop questions. Under GPT-4o-mini, Full-Context lags b\", \"timestamp\": \"2025-12-27T15:47:33.236547Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n",
      "Error running evaluator <DynamicRunEvaluator _wrapped_evaluator> on run 019b607e-3d5e-7ef2-b5ba-5ac7c2637426: GoogleAuthError(\"Unable to find your project. Please provide a project ID by:\\n- Passing a constructor argument\\n- Using vertexai.init()\\n- Setting project using 'gcloud config set project my-project'\\n- Setting a GCP environment variable\\n- To create a Google Cloud project, please follow guidance at https://developers.google.com/workspace/guides/create-project\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/initializer.py\", line 367, in project\n",
      "    self._set_project_as_env_var_or_google_auth_default()\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/initializer.py\", line 114, in _set_project_as_env_var_or_google_auth_default\n",
      "    credentials, project = google.auth.default()\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/auth/_default.py\", line 739, in default\n",
      "    raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1585, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 353, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 779, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/llm.py\", line 566, in _wrapped_evaluator\n",
      "    res = _run_evaluator_untyped(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/utils.py\", line 219, in _run_evaluator_untyped\n",
      "    results = _run_scorer(**kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/utils.py\", line 132, in _run_scorer\n",
      "    score = scorer(**kwargs)\n",
      "            ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/llm.py\", line 238, in get_score\n",
      "    judge = init_chat_model(model=model)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain/chat_models/base.py\", line 316, in init_chat_model\n",
      "    return _init_chat_model_helper(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain/chat_models/base.py\", line 368, in _init_chat_model_helper\n",
      "    return ChatVertexAI(model=model, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 239, in warn_if_direct_instance\n",
      "    return wrapped(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_google_vertexai/chat_models.py\", line 1831, in __init__\n",
      "    super().__init__(**kwargs)\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_core/load/serializable.py\", line 116, in __init__\n",
      "    super().__init__(*args, **kwargs)\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/pydantic/main.py\", line 250, in __init__\n",
      "    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_google_vertexai/_base.py\", line 156, in validate_project\n",
      "    self.project = initializer.global_config.project\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/initializer.py\", line 370, in project\n",
      "    raise GoogleAuthError(project_not_found_exception_str) from exc\n",
      "google.auth.exceptions.GoogleAuthError: Unable to find your project. Please provide a project ID by:\n",
      "- Passing a constructor argument\n",
      "- Using vertexai.init()\n",
      "- Setting project using 'gcloud config set project my-project'\n",
      "- Setting a GCP environment variable\n",
      "- To create a Google Cloud project, please follow guidance at https://developers.google.com/workspace/guides/create-project\n",
      "3it [01:14, 23.52s/it]{\"timestamp\": \"2025-12-27T15:47:45.255252Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"session_id\": \"session_20251227_211745_fc55d6c0\", \"temp_dir\": \"data/session_20251227_211745_fc55d6c0\", \"faiss_dir\": \"faiss_index/session_20251227_211745_fc55d6c0\", \"sessionized\": true, \"timestamp\": \"2025-12-27T15:47:45.257127Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"original_filename\": \"doc.txt\", \"saved_as\": \"data/session_20251227_211745_fc55d6c0/doc_da96b31a.txt\", \"timestamp\": \"2025-12-27T15:47:45.258930Z\", \"level\": \"info\", \"event\": \"File saved successfully\"}\n",
      "{\"count\": 1, \"timestamp\": \"2025-12-27T15:47:45.267882Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 82, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-12-27T15:47:45.278663Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "{\"added\": 1, \"index\": \"faiss_index/session_20251227_211745_fc55d6c0\", \"timestamp\": \"2025-12-27T15:47:47.917939Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2025-12-27T15:47:47.918873Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2025-12-27T15:47:47.924449Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"provider\": \"google\", \"model\": \"gemini-2.5-flash-lite\", \"timestamp\": \"2025-12-27T15:47:47.932631Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20251227_211745_fc55d6c0\", \"timestamp\": \"2025-12-27T15:47:48.117091Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20251227_211745_fc55d6c0\", \"timestamp\": \"2025-12-27T15:47:48.117645Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2025-12-27T15:47:48.121782Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"session_id\": \"session_20251227_211745_fc55d6c0\", \"timestamp\": \"2025-12-27T15:47:48.247524Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20251227_211745_fc55d6c0\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20251227_211745_fc55d6c0\", \"timestamp\": \"2025-12-27T15:47:48.249749Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "AFC is enabled with max remote calls: 10.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "AFC is enabled with max remote calls: 10.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n",
      "{\"session_id\": \"session_20251227_211745_fc55d6c0\", \"user_input\": \"Which question types benefit the most from MemR3?\", \"answer_preview\": \"MemR3 consistently improves over its backbones for open-domain questions. It demonstrates significant gains when using GPT-4.1-mini, nearly matching t\", \"timestamp\": \"2025-12-27T15:47:51.200423Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n",
      "Error running evaluator <DynamicRunEvaluator _wrapped_evaluator> on run 019b607e-8de2-7862-a10f-a766bee4e53c: GoogleAuthError(\"Unable to find your project. Please provide a project ID by:\\n- Passing a constructor argument\\n- Using vertexai.init()\\n- Setting project using 'gcloud config set project my-project'\\n- Setting a GCP environment variable\\n- To create a Google Cloud project, please follow guidance at https://developers.google.com/workspace/guides/create-project\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/initializer.py\", line 367, in project\n",
      "    self._set_project_as_env_var_or_google_auth_default()\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/initializer.py\", line 114, in _set_project_as_env_var_or_google_auth_default\n",
      "    credentials, project = google.auth.default()\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/auth/_default.py\", line 739, in default\n",
      "    raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1585, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 353, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 779, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/llm.py\", line 566, in _wrapped_evaluator\n",
      "    res = _run_evaluator_untyped(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/utils.py\", line 219, in _run_evaluator_untyped\n",
      "    results = _run_scorer(**kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/utils.py\", line 132, in _run_scorer\n",
      "    score = scorer(**kwargs)\n",
      "            ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/llm.py\", line 238, in get_score\n",
      "    judge = init_chat_model(model=model)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain/chat_models/base.py\", line 316, in init_chat_model\n",
      "    return _init_chat_model_helper(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain/chat_models/base.py\", line 368, in _init_chat_model_helper\n",
      "    return ChatVertexAI(model=model, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 239, in warn_if_direct_instance\n",
      "    return wrapped(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_google_vertexai/chat_models.py\", line 1831, in __init__\n",
      "    super().__init__(**kwargs)\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_core/load/serializable.py\", line 116, in __init__\n",
      "    super().__init__(*args, **kwargs)\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/pydantic/main.py\", line 250, in __init__\n",
      "    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_google_vertexai/_base.py\", line 156, in validate_project\n",
      "    self.project = initializer.global_config.project\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/initializer.py\", line 370, in project\n",
      "    raise GoogleAuthError(project_not_found_exception_str) from exc\n",
      "google.auth.exceptions.GoogleAuthError: Unable to find your project. Please provide a project ID by:\n",
      "- Passing a constructor argument\n",
      "- Using vertexai.init()\n",
      "- Setting project using 'gcloud config set project my-project'\n",
      "- Setting a GCP environment variable\n",
      "- To create a Google Cloud project, please follow guidance at https://developers.google.com/workspace/guides/create-project\n",
      "4it [01:32, 21.30s/it]{\"timestamp\": \"2025-12-27T15:48:03.171298Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"session_id\": \"session_20251227_211803_248d950b\", \"temp_dir\": \"data/session_20251227_211803_248d950b\", \"faiss_dir\": \"faiss_index/session_20251227_211803_248d950b\", \"sessionized\": true, \"timestamp\": \"2025-12-27T15:48:03.175477Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"original_filename\": \"doc.txt\", \"saved_as\": \"data/session_20251227_211803_248d950b/doc_3c547452.txt\", \"timestamp\": \"2025-12-27T15:48:03.179570Z\", \"level\": \"info\", \"event\": \"File saved successfully\"}\n",
      "{\"count\": 1, \"timestamp\": \"2025-12-27T15:48:03.181381Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 82, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-12-27T15:48:03.187285Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "{\"added\": 1, \"index\": \"faiss_index/session_20251227_211803_248d950b\", \"timestamp\": \"2025-12-27T15:48:05.937420Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2025-12-27T15:48:05.939420Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2025-12-27T15:48:05.947824Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"provider\": \"google\", \"model\": \"gemini-2.5-flash-lite\", \"timestamp\": \"2025-12-27T15:48:05.948939Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20251227_211803_248d950b\", \"timestamp\": \"2025-12-27T15:48:06.035648Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20251227_211803_248d950b\", \"timestamp\": \"2025-12-27T15:48:06.036272Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2025-12-27T15:48:06.039770Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"session_id\": \"session_20251227_211803_248d950b\", \"timestamp\": \"2025-12-27T15:48:06.127792Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20251227_211803_248d950b\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20251227_211803_248d950b\", \"timestamp\": \"2025-12-27T15:48:06.128349Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "AFC is enabled with max remote calls: 10.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "AFC is enabled with max remote calls: 10.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n",
      "{\"session_id\": \"session_20251227_211803_248d950b\", \"user_input\": \"Why is MemR3 considered plug-and-play?\", \"answer_preview\": \"MemR3 is considered plug-and-play because it treats all concrete retrievers as plug-in modules. Any retriever, such as vector search, graph memory, or\", \"timestamp\": \"2025-12-27T15:48:09.722087Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n",
      "Error running evaluator <DynamicRunEvaluator _wrapped_evaluator> on run 019b607e-d3db-7bc2-b491-3d67b7554e91: GoogleAuthError(\"Unable to find your project. Please provide a project ID by:\\n- Passing a constructor argument\\n- Using vertexai.init()\\n- Setting project using 'gcloud config set project my-project'\\n- Setting a GCP environment variable\\n- To create a Google Cloud project, please follow guidance at https://developers.google.com/workspace/guides/create-project\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/initializer.py\", line 367, in project\n",
      "    self._set_project_as_env_var_or_google_auth_default()\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/initializer.py\", line 114, in _set_project_as_env_var_or_google_auth_default\n",
      "    credentials, project = google.auth.default()\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/auth/_default.py\", line 739, in default\n",
      "    raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1585, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 353, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 779, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/llm.py\", line 566, in _wrapped_evaluator\n",
      "    res = _run_evaluator_untyped(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/utils.py\", line 219, in _run_evaluator_untyped\n",
      "    results = _run_scorer(**kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/utils.py\", line 132, in _run_scorer\n",
      "    score = scorer(**kwargs)\n",
      "            ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/llm.py\", line 238, in get_score\n",
      "    judge = init_chat_model(model=model)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain/chat_models/base.py\", line 316, in init_chat_model\n",
      "    return _init_chat_model_helper(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain/chat_models/base.py\", line 368, in _init_chat_model_helper\n",
      "    return ChatVertexAI(model=model, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 239, in warn_if_direct_instance\n",
      "    return wrapped(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_google_vertexai/chat_models.py\", line 1831, in __init__\n",
      "    super().__init__(**kwargs)\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_core/load/serializable.py\", line 116, in __init__\n",
      "    super().__init__(*args, **kwargs)\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/pydantic/main.py\", line 250, in __init__\n",
      "    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_google_vertexai/_base.py\", line 156, in validate_project\n",
      "    self.project = initializer.global_config.project\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/initializer.py\", line 370, in project\n",
      "    raise GoogleAuthError(project_not_found_exception_str) from exc\n",
      "google.auth.exceptions.GoogleAuthError: Unable to find your project. Please provide a project ID by:\n",
      "- Passing a constructor argument\n",
      "- Using vertexai.init()\n",
      "- Setting project using 'gcloud config set project my-project'\n",
      "- Setting a GCP environment variable\n",
      "- To create a Google Cloud project, please follow guidance at https://developers.google.com/workspace/guides/create-project\n",
      "5it [01:50, 20.40s/it]{\"timestamp\": \"2025-12-27T15:48:21.968289Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"session_id\": \"session_20251227_211821_7f1fd606\", \"temp_dir\": \"data/session_20251227_211821_7f1fd606\", \"faiss_dir\": \"faiss_index/session_20251227_211821_7f1fd606\", \"sessionized\": true, \"timestamp\": \"2025-12-27T15:48:21.969993Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"original_filename\": \"doc.txt\", \"saved_as\": \"data/session_20251227_211821_7f1fd606/doc_2deb2c12.txt\", \"timestamp\": \"2025-12-27T15:48:21.972942Z\", \"level\": \"info\", \"event\": \"File saved successfully\"}\n",
      "{\"count\": 1, \"timestamp\": \"2025-12-27T15:48:21.976734Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 82, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-12-27T15:48:21.984294Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "{\"added\": 1, \"index\": \"faiss_index/session_20251227_211821_7f1fd606\", \"timestamp\": \"2025-12-27T15:48:24.574631Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2025-12-27T15:48:24.576687Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2025-12-27T15:48:24.587625Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"provider\": \"google\", \"model\": \"gemini-2.5-flash-lite\", \"timestamp\": \"2025-12-27T15:48:24.589339Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20251227_211821_7f1fd606\", \"timestamp\": \"2025-12-27T15:48:24.756435Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20251227_211821_7f1fd606\", \"timestamp\": \"2025-12-27T15:48:24.759798Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2025-12-27T15:48:24.762007Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"session_id\": \"session_20251227_211821_7f1fd606\", \"timestamp\": \"2025-12-27T15:48:24.868414Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20251227_211821_7f1fd606\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20251227_211821_7f1fd606\", \"timestamp\": \"2025-12-27T15:48:24.868963Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "AFC is enabled with max remote calls: 10.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "AFC is enabled with max remote calls: 10.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n",
      "{\"session_id\": \"session_20251227_211821_7f1fd606\", \"user_input\": \"What actions can the MemR3 router choose from?\", \"answer_preview\": \"The MemR3 router can choose from three actions: retrieve, reflect, and answer. Each action is accompanied by a textual generation that provides furthe\", \"timestamp\": \"2025-12-27T15:48:27.645990Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n",
      "Error running evaluator <DynamicRunEvaluator _wrapped_evaluator> on run 019b607f-1d4a-7c92-8380-9fc82d8ef69a: GoogleAuthError(\"Unable to find your project. Please provide a project ID by:\\n- Passing a constructor argument\\n- Using vertexai.init()\\n- Setting project using 'gcloud config set project my-project'\\n- Setting a GCP environment variable\\n- To create a Google Cloud project, please follow guidance at https://developers.google.com/workspace/guides/create-project\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/initializer.py\", line 367, in project\n",
      "    self._set_project_as_env_var_or_google_auth_default()\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/initializer.py\", line 114, in _set_project_as_env_var_or_google_auth_default\n",
      "    credentials, project = google.auth.default()\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/auth/_default.py\", line 739, in default\n",
      "    raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1585, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 353, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 779, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/llm.py\", line 566, in _wrapped_evaluator\n",
      "    res = _run_evaluator_untyped(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/utils.py\", line 219, in _run_evaluator_untyped\n",
      "    results = _run_scorer(**kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/utils.py\", line 132, in _run_scorer\n",
      "    score = scorer(**kwargs)\n",
      "            ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/llm.py\", line 238, in get_score\n",
      "    judge = init_chat_model(model=model)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain/chat_models/base.py\", line 316, in init_chat_model\n",
      "    return _init_chat_model_helper(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain/chat_models/base.py\", line 368, in _init_chat_model_helper\n",
      "    return ChatVertexAI(model=model, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 239, in warn_if_direct_instance\n",
      "    return wrapped(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_google_vertexai/chat_models.py\", line 1831, in __init__\n",
      "    super().__init__(**kwargs)\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_core/load/serializable.py\", line 116, in __init__\n",
      "    super().__init__(*args, **kwargs)\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/pydantic/main.py\", line 250, in __init__\n",
      "    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_google_vertexai/_base.py\", line 156, in validate_project\n",
      "    self.project = initializer.global_config.project\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/initializer.py\", line 370, in project\n",
      "    raise GoogleAuthError(project_not_found_exception_str) from exc\n",
      "google.auth.exceptions.GoogleAuthError: Unable to find your project. Please provide a project ID by:\n",
      "- Passing a constructor argument\n",
      "- Using vertexai.init()\n",
      "- Setting project using 'gcloud config set project my-project'\n",
      "- Setting a GCP environment variable\n",
      "- To create a Google Cloud project, please follow guidance at https://developers.google.com/workspace/guides/create-project\n",
      "6it [02:08, 19.52s/it]{\"timestamp\": \"2025-12-27T15:48:39.794777Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"session_id\": \"session_20251227_211839_3a82268d\", \"temp_dir\": \"data/session_20251227_211839_3a82268d\", \"faiss_dir\": \"faiss_index/session_20251227_211839_3a82268d\", \"sessionized\": true, \"timestamp\": \"2025-12-27T15:48:39.796738Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"original_filename\": \"doc.txt\", \"saved_as\": \"data/session_20251227_211839_3a82268d/doc_1c5d663e.txt\", \"timestamp\": \"2025-12-27T15:48:39.798486Z\", \"level\": \"info\", \"event\": \"File saved successfully\"}\n",
      "{\"count\": 1, \"timestamp\": \"2025-12-27T15:48:39.800218Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 82, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-12-27T15:48:39.805522Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "{\"added\": 1, \"index\": \"faiss_index/session_20251227_211839_3a82268d\", \"timestamp\": \"2025-12-27T15:48:42.521636Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2025-12-27T15:48:42.530234Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2025-12-27T15:48:42.548112Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"provider\": \"google\", \"model\": \"gemini-2.5-flash-lite\", \"timestamp\": \"2025-12-27T15:48:42.550749Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20251227_211839_3a82268d\", \"timestamp\": \"2025-12-27T15:48:42.667360Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20251227_211839_3a82268d\", \"timestamp\": \"2025-12-27T15:48:42.667925Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2025-12-27T15:48:42.672290Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"session_id\": \"session_20251227_211839_3a82268d\", \"timestamp\": \"2025-12-27T15:48:42.801496Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20251227_211839_3a82268d\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20251227_211839_3a82268d\", \"timestamp\": \"2025-12-27T15:48:42.802586Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "AFC is enabled with max remote calls: 10.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "AFC is enabled with max remote calls: 10.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n",
      "{\"session_id\": \"session_20251227_211839_3a82268d\", \"user_input\": \"What problem does MemR3 aim to solve in LLM memory systems?\", \"answer_preview\": \"MemR3 aims to solve problems of noisy recall, heavy retrieval, and increasing latency in LLM memory systems. It seeks to improve retrieval quality and\", \"timestamp\": \"2025-12-27T15:48:45.367783Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n",
      "Error running evaluator <DynamicRunEvaluator _wrapped_evaluator> on run 019b607f-62e9-7ac1-8b41-324e5674d57f: GoogleAuthError(\"Unable to find your project. Please provide a project ID by:\\n- Passing a constructor argument\\n- Using vertexai.init()\\n- Setting project using 'gcloud config set project my-project'\\n- Setting a GCP environment variable\\n- To create a Google Cloud project, please follow guidance at https://developers.google.com/workspace/guides/create-project\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/initializer.py\", line 367, in project\n",
      "    self._set_project_as_env_var_or_google_auth_default()\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/initializer.py\", line 114, in _set_project_as_env_var_or_google_auth_default\n",
      "    credentials, project = google.auth.default()\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/auth/_default.py\", line 739, in default\n",
      "    raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1585, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 353, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 779, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/llm.py\", line 566, in _wrapped_evaluator\n",
      "    res = _run_evaluator_untyped(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/utils.py\", line 219, in _run_evaluator_untyped\n",
      "    results = _run_scorer(**kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/utils.py\", line 132, in _run_scorer\n",
      "    score = scorer(**kwargs)\n",
      "            ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/llm.py\", line 238, in get_score\n",
      "    judge = init_chat_model(model=model)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain/chat_models/base.py\", line 316, in init_chat_model\n",
      "    return _init_chat_model_helper(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain/chat_models/base.py\", line 368, in _init_chat_model_helper\n",
      "    return ChatVertexAI(model=model, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 239, in warn_if_direct_instance\n",
      "    return wrapped(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_google_vertexai/chat_models.py\", line 1831, in __init__\n",
      "    super().__init__(**kwargs)\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_core/load/serializable.py\", line 116, in __init__\n",
      "    super().__init__(*args, **kwargs)\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/pydantic/main.py\", line 250, in __init__\n",
      "    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_google_vertexai/_base.py\", line 156, in validate_project\n",
      "    self.project = initializer.global_config.project\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/initializer.py\", line 370, in project\n",
      "    raise GoogleAuthError(project_not_found_exception_str) from exc\n",
      "google.auth.exceptions.GoogleAuthError: Unable to find your project. Please provide a project ID by:\n",
      "- Passing a constructor argument\n",
      "- Using vertexai.init()\n",
      "- Setting project using 'gcloud config set project my-project'\n",
      "- Setting a GCP environment variable\n",
      "- To create a Google Cloud project, please follow guidance at https://developers.google.com/workspace/guides/create-project\n",
      "7it [02:26, 18.86s/it]{\"timestamp\": \"2025-12-27T15:48:57.297583Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"session_id\": \"session_20251227_211857_bf029eeb\", \"temp_dir\": \"data/session_20251227_211857_bf029eeb\", \"faiss_dir\": \"faiss_index/session_20251227_211857_bf029eeb\", \"sessionized\": true, \"timestamp\": \"2025-12-27T15:48:57.301185Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"original_filename\": \"doc.txt\", \"saved_as\": \"data/session_20251227_211857_bf029eeb/doc_4d56f9a5.txt\", \"timestamp\": \"2025-12-27T15:48:57.303856Z\", \"level\": \"info\", \"event\": \"File saved successfully\"}\n",
      "{\"count\": 1, \"timestamp\": \"2025-12-27T15:48:57.309919Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 82, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-12-27T15:48:57.319587Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "{\"added\": 1, \"index\": \"faiss_index/session_20251227_211857_bf029eeb\", \"timestamp\": \"2025-12-27T15:49:00.107802Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2025-12-27T15:49:00.112164Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2025-12-27T15:49:00.134383Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"provider\": \"google\", \"model\": \"gemini-2.5-flash-lite\", \"timestamp\": \"2025-12-27T15:49:00.136477Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20251227_211857_bf029eeb\", \"timestamp\": \"2025-12-27T15:49:00.265173Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20251227_211857_bf029eeb\", \"timestamp\": \"2025-12-27T15:49:00.265841Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2025-12-27T15:49:00.268506Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"session_id\": \"session_20251227_211857_bf029eeb\", \"timestamp\": \"2025-12-27T15:49:00.402040Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20251227_211857_bf029eeb\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20251227_211857_bf029eeb\", \"timestamp\": \"2025-12-27T15:49:00.404039Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "AFC is enabled with max remote calls: 10.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "AFC is enabled with max remote calls: 10.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n",
      "{\"session_id\": \"session_20251227_211857_bf029eeb\", \"user_input\": \"On which benchmark was MemR3 evaluated?\", \"answer_preview\": \"MemR3 was evaluated on the LoCoMo dataset. This dataset includes conversations across four categories: multi-hop, temporal, open-domain, and single-ho\", \"timestamp\": \"2025-12-27T15:49:03.129225Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n",
      "Error running evaluator <DynamicRunEvaluator _wrapped_evaluator> on run 019b607f-a746-7eb2-b59f-520a80fac94d: GoogleAuthError(\"Unable to find your project. Please provide a project ID by:\\n- Passing a constructor argument\\n- Using vertexai.init()\\n- Setting project using 'gcloud config set project my-project'\\n- Setting a GCP environment variable\\n- To create a Google Cloud project, please follow guidance at https://developers.google.com/workspace/guides/create-project\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/initializer.py\", line 367, in project\n",
      "    self._set_project_as_env_var_or_google_auth_default()\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/initializer.py\", line 114, in _set_project_as_env_var_or_google_auth_default\n",
      "    credentials, project = google.auth.default()\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/auth/_default.py\", line 739, in default\n",
      "    raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1585, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 353, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 779, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/llm.py\", line 566, in _wrapped_evaluator\n",
      "    res = _run_evaluator_untyped(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/utils.py\", line 219, in _run_evaluator_untyped\n",
      "    results = _run_scorer(**kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/utils.py\", line 132, in _run_scorer\n",
      "    score = scorer(**kwargs)\n",
      "            ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/llm.py\", line 238, in get_score\n",
      "    judge = init_chat_model(model=model)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain/chat_models/base.py\", line 316, in init_chat_model\n",
      "    return _init_chat_model_helper(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain/chat_models/base.py\", line 368, in _init_chat_model_helper\n",
      "    return ChatVertexAI(model=model, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 239, in warn_if_direct_instance\n",
      "    return wrapped(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_google_vertexai/chat_models.py\", line 1831, in __init__\n",
      "    super().__init__(**kwargs)\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_core/load/serializable.py\", line 116, in __init__\n",
      "    super().__init__(*args, **kwargs)\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/pydantic/main.py\", line 250, in __init__\n",
      "    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_google_vertexai/_base.py\", line 156, in validate_project\n",
      "    self.project = initializer.global_config.project\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/initializer.py\", line 370, in project\n",
      "    raise GoogleAuthError(project_not_found_exception_str) from exc\n",
      "google.auth.exceptions.GoogleAuthError: Unable to find your project. Please provide a project ID by:\n",
      "- Passing a constructor argument\n",
      "- Using vertexai.init()\n",
      "- Setting project using 'gcloud config set project my-project'\n",
      "- Setting a GCP environment variable\n",
      "- To create a Google Cloud project, please follow guidance at https://developers.google.com/workspace/guides/create-project\n",
      "8it [02:44, 18.53s/it]{\"timestamp\": \"2025-12-27T15:49:15.115853Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"session_id\": \"session_20251227_211915_bbead225\", \"temp_dir\": \"data/session_20251227_211915_bbead225\", \"faiss_dir\": \"faiss_index/session_20251227_211915_bbead225\", \"sessionized\": true, \"timestamp\": \"2025-12-27T15:49:15.116867Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"original_filename\": \"doc.txt\", \"saved_as\": \"data/session_20251227_211915_bbead225/doc_924d2633.txt\", \"timestamp\": \"2025-12-27T15:49:15.117886Z\", \"level\": \"info\", \"event\": \"File saved successfully\"}\n",
      "{\"count\": 1, \"timestamp\": \"2025-12-27T15:49:15.118756Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 82, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-12-27T15:49:15.121963Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "{\"added\": 1, \"index\": \"faiss_index/session_20251227_211915_bbead225\", \"timestamp\": \"2025-12-27T15:49:17.613888Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2025-12-27T15:49:17.615572Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2025-12-27T15:49:17.626580Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"provider\": \"google\", \"model\": \"gemini-2.5-flash-lite\", \"timestamp\": \"2025-12-27T15:49:17.629572Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20251227_211915_bbead225\", \"timestamp\": \"2025-12-27T15:49:17.718032Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20251227_211915_bbead225\", \"timestamp\": \"2025-12-27T15:49:17.719106Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2025-12-27T15:49:17.723548Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"session_id\": \"session_20251227_211915_bbead225\", \"timestamp\": \"2025-12-27T15:49:17.814822Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20251227_211915_bbead225\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20251227_211915_bbead225\", \"timestamp\": \"2025-12-27T15:49:17.815391Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "AFC is enabled with max remote calls: 10.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "AFC is enabled with max remote calls: 10.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n",
      "{\"session_id\": \"session_20251227_211915_bbead225\", \"user_input\": \"What are the two core mechanisms of MemR3?\", \"answer_preview\": \"The two core mechanisms of MemR3 are maintaining an explicit evidence-gap state and interleaving retrieval with reflection. The evidence-gap state sep\", \"timestamp\": \"2025-12-27T15:49:20.541096Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n",
      "Error running evaluator <DynamicRunEvaluator _wrapped_evaluator> on run 019b607f-ece9-7780-a96d-2473435d1585: GoogleAuthError(\"Unable to find your project. Please provide a project ID by:\\n- Passing a constructor argument\\n- Using vertexai.init()\\n- Setting project using 'gcloud config set project my-project'\\n- Setting a GCP environment variable\\n- To create a Google Cloud project, please follow guidance at https://developers.google.com/workspace/guides/create-project\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/initializer.py\", line 367, in project\n",
      "    self._set_project_as_env_var_or_google_auth_default()\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/initializer.py\", line 114, in _set_project_as_env_var_or_google_auth_default\n",
      "    credentials, project = google.auth.default()\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/auth/_default.py\", line 739, in default\n",
      "    raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1585, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 353, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 779, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/llm.py\", line 566, in _wrapped_evaluator\n",
      "    res = _run_evaluator_untyped(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/utils.py\", line 219, in _run_evaluator_untyped\n",
      "    results = _run_scorer(**kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/utils.py\", line 132, in _run_scorer\n",
      "    score = scorer(**kwargs)\n",
      "            ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/llm.py\", line 238, in get_score\n",
      "    judge = init_chat_model(model=model)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain/chat_models/base.py\", line 316, in init_chat_model\n",
      "    return _init_chat_model_helper(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain/chat_models/base.py\", line 368, in _init_chat_model_helper\n",
      "    return ChatVertexAI(model=model, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 239, in warn_if_direct_instance\n",
      "    return wrapped(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_google_vertexai/chat_models.py\", line 1831, in __init__\n",
      "    super().__init__(**kwargs)\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_core/load/serializable.py\", line 116, in __init__\n",
      "    super().__init__(*args, **kwargs)\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/pydantic/main.py\", line 250, in __init__\n",
      "    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_google_vertexai/_base.py\", line 156, in validate_project\n",
      "    self.project = initializer.global_config.project\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/initializer.py\", line 370, in project\n",
      "    raise GoogleAuthError(project_not_found_exception_str) from exc\n",
      "google.auth.exceptions.GoogleAuthError: Unable to find your project. Please provide a project ID by:\n",
      "- Passing a constructor argument\n",
      "- Using vertexai.init()\n",
      "- Setting project using 'gcloud config set project my-project'\n",
      "- Setting a GCP environment variable\n",
      "- To create a Google Cloud project, please follow guidance at https://developers.google.com/workspace/guides/create-project\n",
      "9it [03:01, 18.15s/it]{\"timestamp\": \"2025-12-27T15:49:32.438303Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"session_id\": \"session_20251227_211932_fe308335\", \"temp_dir\": \"data/session_20251227_211932_fe308335\", \"faiss_dir\": \"faiss_index/session_20251227_211932_fe308335\", \"sessionized\": true, \"timestamp\": \"2025-12-27T15:49:32.446583Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"original_filename\": \"doc.txt\", \"saved_as\": \"data/session_20251227_211932_fe308335/doc_5b80aa5c.txt\", \"timestamp\": \"2025-12-27T15:49:32.448926Z\", \"level\": \"info\", \"event\": \"File saved successfully\"}\n",
      "{\"count\": 1, \"timestamp\": \"2025-12-27T15:49:32.450525Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 82, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-12-27T15:49:32.459391Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "{\"added\": 1, \"index\": \"faiss_index/session_20251227_211932_fe308335\", \"timestamp\": \"2025-12-27T15:49:35.243347Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2025-12-27T15:49:35.247115Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2025-12-27T15:49:35.261877Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"provider\": \"google\", \"model\": \"gemini-2.5-flash-lite\", \"timestamp\": \"2025-12-27T15:49:35.264781Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20251227_211932_fe308335\", \"timestamp\": \"2025-12-27T15:49:35.449159Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20251227_211932_fe308335\", \"timestamp\": \"2025-12-27T15:49:35.449978Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2025-12-27T15:49:35.454389Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"session_id\": \"session_20251227_211932_fe308335\", \"timestamp\": \"2025-12-27T15:49:35.598311Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20251227_211932_fe308335\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20251227_211932_fe308335\", \"timestamp\": \"2025-12-27T15:49:35.599093Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "AFC is enabled with max remote calls: 10.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "AFC is enabled with max remote calls: 10.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n",
      "{\"session_id\": \"session_20251227_211932_fe308335\", \"user_input\": \"How does MemR3 differ from the standard retrieve-then-answer pipeline?\", \"answer_preview\": \"MemR3 introduces a closed-loop controller that can refine retrieval queries and integrate new evidence, unlike the classical retrieve-then-answer pipe\", \"timestamp\": \"2025-12-27T15:49:39.117520Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n",
      "Error running evaluator <DynamicRunEvaluator _wrapped_evaluator> on run 019b6080-3090-7743-9229-c2019d57baf7: GoogleAuthError(\"Unable to find your project. Please provide a project ID by:\\n- Passing a constructor argument\\n- Using vertexai.init()\\n- Setting project using 'gcloud config set project my-project'\\n- Setting a GCP environment variable\\n- To create a Google Cloud project, please follow guidance at https://developers.google.com/workspace/guides/create-project\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/initializer.py\", line 367, in project\n",
      "    self._set_project_as_env_var_or_google_auth_default()\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/initializer.py\", line 114, in _set_project_as_env_var_or_google_auth_default\n",
      "    credentials, project = google.auth.default()\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/auth/_default.py\", line 739, in default\n",
      "    raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1585, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 353, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 779, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/llm.py\", line 566, in _wrapped_evaluator\n",
      "    res = _run_evaluator_untyped(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/utils.py\", line 219, in _run_evaluator_untyped\n",
      "    results = _run_scorer(**kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/utils.py\", line 132, in _run_scorer\n",
      "    score = scorer(**kwargs)\n",
      "            ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/llm.py\", line 238, in get_score\n",
      "    judge = init_chat_model(model=model)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain/chat_models/base.py\", line 316, in init_chat_model\n",
      "    return _init_chat_model_helper(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain/chat_models/base.py\", line 368, in _init_chat_model_helper\n",
      "    return ChatVertexAI(model=model, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 239, in warn_if_direct_instance\n",
      "    return wrapped(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_google_vertexai/chat_models.py\", line 1831, in __init__\n",
      "    super().__init__(**kwargs)\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_core/load/serializable.py\", line 116, in __init__\n",
      "    super().__init__(*args, **kwargs)\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/pydantic/main.py\", line 250, in __init__\n",
      "    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_google_vertexai/_base.py\", line 156, in validate_project\n",
      "    self.project = initializer.global_config.project\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/initializer.py\", line 370, in project\n",
      "    raise GoogleAuthError(project_not_found_exception_str) from exc\n",
      "google.auth.exceptions.GoogleAuthError: Unable to find your project. Please provide a project ID by:\n",
      "- Passing a constructor argument\n",
      "- Using vertexai.init()\n",
      "- Setting project using 'gcloud config set project my-project'\n",
      "- Setting a GCP environment variable\n",
      "- To create a Google Cloud project, please follow guidance at https://developers.google.com/workspace/guides/create-project\n",
      "10it [03:20, 18.37s/it]{\"timestamp\": \"2025-12-27T15:49:51.298299Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"session_id\": \"session_20251227_211951_fac92028\", \"temp_dir\": \"data/session_20251227_211951_fac92028\", \"faiss_dir\": \"faiss_index/session_20251227_211951_fac92028\", \"sessionized\": true, \"timestamp\": \"2025-12-27T15:49:51.299852Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"original_filename\": \"doc.txt\", \"saved_as\": \"data/session_20251227_211951_fac92028/doc_629893fc.txt\", \"timestamp\": \"2025-12-27T15:49:51.303233Z\", \"level\": \"info\", \"event\": \"File saved successfully\"}\n",
      "{\"count\": 1, \"timestamp\": \"2025-12-27T15:49:51.308658Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 82, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-12-27T15:49:51.314296Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "{\"added\": 1, \"index\": \"faiss_index/session_20251227_211951_fac92028\", \"timestamp\": \"2025-12-27T15:49:53.671432Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2025-12-27T15:49:53.677992Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2025-12-27T15:49:53.691849Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"provider\": \"google\", \"model\": \"gemini-2.5-flash-lite\", \"timestamp\": \"2025-12-27T15:49:53.693852Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20251227_211951_fac92028\", \"timestamp\": \"2025-12-27T15:49:53.795612Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20251227_211951_fac92028\", \"timestamp\": \"2025-12-27T15:49:53.796162Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2025-12-27T15:49:53.798485Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"session_id\": \"session_20251227_211951_fac92028\", \"timestamp\": \"2025-12-27T15:49:53.882090Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20251227_211951_fac92028\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20251227_211951_fac92028\", \"timestamp\": \"2025-12-27T15:49:53.882660Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "AFC is enabled with max remote calls: 10.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "AFC is enabled with max remote calls: 10.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying google.genai._api_client.BaseApiClient._request_once in 1.7520302801005214 seconds as it raised ClientError: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash-lite\\nPlease retry in 3.948911636s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.5-flash-lite', 'location': 'global'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '3s'}]}}.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying google.genai._api_client.BaseApiClient._request_once in 2.129835724939155 seconds as it raised ClientError: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash-lite\\nPlease retry in 2.073794826s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.5-flash-lite', 'location': 'global'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '2s'}]}}.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying google.genai._api_client.BaseApiClient._request_once in 4.606824192151066 seconds as it raised ClientError: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash-lite\\nPlease retry in 59.821161968s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-lite'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '59s'}]}}.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying google.genai._api_client.BaseApiClient._request_once in 8.411062564145162 seconds as it raised ClientError: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash-lite\\nPlease retry in 55.091774995s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-lite'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '55s'}]}}.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying google.genai._api_client.BaseApiClient._request_once in 16.137043457891863 seconds as it raised ClientError: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash-lite\\nPlease retry in 46.230338312s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-lite'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '46s'}]}}.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 429 Too Many Requests\"\n",
      "{\"error\": \"Error calling model 'gemini-2.5-flash-lite' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash-lite\\\\nPlease retry in 29.561332627s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-lite'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '29s'}]}}\", \"timestamp\": \"2025-12-27T15:50:30.374333Z\", \"level\": \"error\", \"event\": \"Failed to invoke ConversationalRAG\"}\n",
      "Error running evaluator <DynamicRunEvaluator _wrapped_evaluator> on run 019b6080-7a3b-7a32-a55f-f6c1e30c1c09: GoogleAuthError(\"Unable to find your project. Please provide a project ID by:\\n- Passing a constructor argument\\n- Using vertexai.init()\\n- Setting project using 'gcloud config set project my-project'\\n- Setting a GCP environment variable\\n- To create a Google Cloud project, please follow guidance at https://developers.google.com/workspace/guides/create-project\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/initializer.py\", line 367, in project\n",
      "    self._set_project_as_env_var_or_google_auth_default()\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/initializer.py\", line 114, in _set_project_as_env_var_or_google_auth_default\n",
      "    credentials, project = google.auth.default()\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/auth/_default.py\", line 739, in default\n",
      "    raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1585, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 353, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 779, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/llm.py\", line 566, in _wrapped_evaluator\n",
      "    res = _run_evaluator_untyped(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/utils.py\", line 219, in _run_evaluator_untyped\n",
      "    results = _run_scorer(**kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/utils.py\", line 132, in _run_scorer\n",
      "    score = scorer(**kwargs)\n",
      "            ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/llm.py\", line 238, in get_score\n",
      "    judge = init_chat_model(model=model)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain/chat_models/base.py\", line 316, in init_chat_model\n",
      "    return _init_chat_model_helper(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain/chat_models/base.py\", line 368, in _init_chat_model_helper\n",
      "    return ChatVertexAI(model=model, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 239, in warn_if_direct_instance\n",
      "    return wrapped(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_google_vertexai/chat_models.py\", line 1831, in __init__\n",
      "    super().__init__(**kwargs)\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_core/load/serializable.py\", line 116, in __init__\n",
      "    super().__init__(*args, **kwargs)\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/pydantic/main.py\", line 250, in __init__\n",
      "    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_google_vertexai/_base.py\", line 156, in validate_project\n",
      "    self.project = initializer.global_config.project\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ayush/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/initializer.py\", line 370, in project\n",
      "    raise GoogleAuthError(project_not_found_exception_str) from exc\n",
      "google.auth.exceptions.GoogleAuthError: Unable to find your project. Please provide a project ID by:\n",
      "- Passing a constructor argument\n",
      "- Using vertexai.init()\n",
      "- Setting project using 'gcloud config set project my-project'\n",
      "- Setting a GCP environment variable\n",
      "- To create a Google Cloud project, please follow guidance at https://developers.google.com/workspace/guides/create-project\n",
      "11it [04:11, 28.37s/it]{\"timestamp\": \"2025-12-27T15:50:42.355882Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"session_id\": \"session_20251227_212042_5a796be1\", \"temp_dir\": \"data/session_20251227_212042_5a796be1\", \"faiss_dir\": \"faiss_index/session_20251227_212042_5a796be1\", \"sessionized\": true, \"timestamp\": \"2025-12-27T15:50:42.357892Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"original_filename\": \"doc.txt\", \"saved_as\": \"data/session_20251227_212042_5a796be1/doc_bd38d218.txt\", \"timestamp\": \"2025-12-27T15:50:42.367613Z\", \"level\": \"info\", \"event\": \"File saved successfully\"}\n",
      "{\"count\": 1, \"timestamp\": \"2025-12-27T15:50:42.372417Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 82, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-12-27T15:50:42.383306Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "{\"added\": 1, \"index\": \"faiss_index/session_20251227_212042_5a796be1\", \"timestamp\": \"2025-12-27T15:50:45.374871Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2025-12-27T15:50:45.377231Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2025-12-27T15:50:45.393060Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"provider\": \"google\", \"model\": \"gemini-2.5-flash-lite\", \"timestamp\": \"2025-12-27T15:50:45.396931Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20251227_212042_5a796be1\", \"timestamp\": \"2025-12-27T15:50:45.552096Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20251227_212042_5a796be1\", \"timestamp\": \"2025-12-27T15:50:45.554796Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2025-12-27T15:50:45.562791Z\", \"level\": \"info\", \"event\": \"Config Yaml loaded : ['embedding_model', 'retriever', 'llm']\"}\n",
      "{\"session_id\": \"session_20251227_212042_5a796be1\", \"timestamp\": \"2025-12-27T15:50:45.732970Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20251227_212042_5a796be1\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20251227_212042_5a796be1\", \"timestamp\": \"2025-12-27T15:50:45.734122Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "AFC is enabled with max remote calls: 10.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying google.genai._api_client.BaseApiClient._request_once in 1.6032602519606272 seconds as it raised ClientError: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash-lite\\nPlease retry in 13.636206113s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.5-flash-lite', 'location': 'global'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '13s'}]}}.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying google.genai._api_client.BaseApiClient._request_once in 2.9017133177220957 seconds as it raised ClientError: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash-lite\\nPlease retry in 11.88665247s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-lite'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '11s'}]}}.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying google.genai._api_client.BaseApiClient._request_once in 4.282771201707297 seconds as it raised ClientError: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash-lite\\nPlease retry in 8.851267394s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-lite'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '8s'}]}}.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying google.genai._api_client.BaseApiClient._request_once in 8.363543082907947 seconds as it raised ClientError: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash-lite\\nPlease retry in 4.435428027s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-lite'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '4s'}]}}.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying google.genai._api_client.BaseApiClient._request_once in 16.394123707573563 seconds as it raised ClientError: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash-lite\\nPlease retry in 55.614201459s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-lite'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '55s'}]}}.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 429 Too Many Requests\"\n",
      "{\"error\": \"Error calling model 'gemini-2.5-flash-lite' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash-lite\\\\nPlease retry in 38.813443886s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-lite'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '38s'}]}}\", \"timestamp\": \"2025-12-27T15:51:21.244813Z\", \"level\": \"error\", \"event\": \"Failed to invoke ConversationalRAG\"}\n",
      "11it [04:54, 26.79s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      5\u001b[39m client = Client()\n\u001b[32m      7\u001b[39m qa_evaluator = create_llm_as_judge(\n\u001b[32m      8\u001b[39m     prompt=CONCISENESS_PROMPT,\n\u001b[32m      9\u001b[39m     feedback_key=\u001b[33m\"\u001b[39m\u001b[33mqa_correctness\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mgemini:model/gemini-2.5-flash-lite\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m experiment_results = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer_ai_report_question\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# ← target MUST be positional\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAgenticAImemoryqa\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# ← data MUST be positional\u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mqa_evaluator\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexperiment_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest-agenticAIReport-qa-rag\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvariant\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRAG with FAISS + agent memory\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchunk_size\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchunk_overlap\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mk\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/client.py:8328\u001b[39m, in \u001b[36mClient.evaluate\u001b[39m\u001b[34m(self, target, data, evaluators, summary_evaluators, metadata, experiment_prefix, description, max_concurrency, num_repetitions, blocking, experiment, upload_results, error_handling, **kwargs)\u001b[39m\n\u001b[32m   8324\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangsmith\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_runner\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluate \u001b[38;5;28;01mas\u001b[39;00m evaluate_\n\u001b[32m   8326\u001b[39m \u001b[38;5;66;03m# Need to ignore because it fails when there are too many union types +\u001b[39;00m\n\u001b[32m   8327\u001b[39m \u001b[38;5;66;03m# overloads.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m8328\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mevaluate_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[misc]\u001b[39;49;00m\n\u001b[32m   8329\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   8330\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   8331\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   8332\u001b[39m \u001b[43m    \u001b[49m\u001b[43msummary_evaluators\u001b[49m\u001b[43m=\u001b[49m\u001b[43msummary_evaluators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   8333\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   8334\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexperiment_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexperiment_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   8335\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   8336\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_concurrency\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_concurrency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   8337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_repetitions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_repetitions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   8338\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   8339\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   8340\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexperiment\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexperiment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   8341\u001b[39m \u001b[43m    \u001b[49m\u001b[43mupload_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mupload_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   8342\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_handling\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_handling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   8343\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   8344\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/_runner.py:389\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(target, data, evaluators, summary_evaluators, metadata, experiment_prefix, description, max_concurrency, num_repetitions, client, blocking, experiment, upload_results, error_handling, **kwargs)\u001b[39m\n\u001b[32m    387\u001b[39m     _warn_once(\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mupload_results\u001b[39m\u001b[33m'\u001b[39m\u001b[33m parameter is in beta.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    388\u001b[39m logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning evaluation over target system \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOptional\u001b[49m\u001b[43m[\u001b[49m\u001b[43mSequence\u001b[49m\u001b[43m[\u001b[49m\u001b[43mEVALUATOR_T\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m    \u001b[49m\u001b[43msummary_evaluators\u001b[49m\u001b[43m=\u001b[49m\u001b[43msummary_evaluators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexperiment_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexperiment_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_concurrency\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_concurrency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_repetitions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_repetitions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexperiment\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexperiment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m    \u001b[49m\u001b[43mupload_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mupload_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_handling\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_handling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/_runner.py:1066\u001b[39m, in \u001b[36m_evaluate\u001b[39m\u001b[34m(target, data, evaluators, summary_evaluators, metadata, experiment_prefix, description, max_concurrency, num_repetitions, client, blocking, experiment, upload_results, error_handling)\u001b[39m\n\u001b[32m   1064\u001b[39m     manager = manager.with_summary_evaluators(summary_evaluators)\n\u001b[32m   1065\u001b[39m \u001b[38;5;66;03m# Start consuming the results.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1066\u001b[39m results = \u001b[43mExperimentResults\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1067\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/_runner.py:549\u001b[39m, in \u001b[36mExperimentResults.__init__\u001b[39m\u001b[34m(self, experiment_manager, blocking)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    548\u001b[39m     \u001b[38;5;28mself\u001b[39m._thread = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/_runner.py:574\u001b[39m, in \u001b[36mExperimentResults._process_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    572\u001b[39m tqdm = _load_tqdm()\n\u001b[32m    573\u001b[39m results = \u001b[38;5;28mself\u001b[39m._manager.get_results()\n\u001b[32m--> \u001b[39m\u001b[32m574\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_results\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/_runner.py:1487\u001b[39m, in \u001b[36m_ExperimentManager.get_results\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1485\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_results\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Iterable[ExperimentResultRow]:\n\u001b[32m   1486\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the traces, evaluation results, and associated examples.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1487\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluation_results\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1488\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mruns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluation_results\u001b[49m\n\u001b[32m   1489\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1490\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mExperimentResultRow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1491\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[43m            \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1493\u001b[39m \u001b[43m            \u001b[49m\u001b[43mevaluation_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluation_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1494\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/_runner.py:1467\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   1462\u001b[39m \u001b[38;5;66;03m# Split the generator into three so the manager\u001b[39;00m\n\u001b[32m   1463\u001b[39m \u001b[38;5;66;03m# can consume each value individually.\u001b[39;00m\n\u001b[32m   1464\u001b[39m r1, r2, r3 = itertools.tee(experiment_results, \u001b[32m3\u001b[39m)\n\u001b[32m   1465\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._copy(\n\u001b[32m   1466\u001b[39m     (result[\u001b[33m\"\u001b[39m\u001b[33mexample\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m r1),\n\u001b[32m-> \u001b[39m\u001b[32m1467\u001b[39m     runs=\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr2\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1468\u001b[39m     evaluation_results=(result[\u001b[33m\"\u001b[39m\u001b[33mevaluation_results\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m r3),\n\u001b[32m   1469\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/_runner.py:1657\u001b[39m, in \u001b[36m_ExperimentManager._score\u001b[39m\u001b[34m(self, evaluators, max_concurrency)\u001b[39m\n\u001b[32m   1655\u001b[39m     context = copy_context()\n\u001b[32m   1656\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m current_results \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_results():\n\u001b[32m-> \u001b[39m\u001b[32m1657\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1658\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_evaluators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1659\u001b[39m \u001b[43m            \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1660\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcurrent_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1661\u001b[39m \u001b[43m            \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1662\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1663\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1664\u001b[39m     futures = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/_runner.py:1585\u001b[39m, in \u001b[36m_ExperimentManager._run_evaluators\u001b[39m\u001b[34m(self, evaluators, current_results, executor)\u001b[39m\n\u001b[32m   1583\u001b[39m evaluator_run_id = uuid.uuid4()\n\u001b[32m   1584\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1585\u001b[39m     evaluator_response = \u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-arg]\u001b[39;49;00m\n\u001b[32m   1586\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1587\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1588\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevaluator_run_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluator_run_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1589\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1591\u001b[39m     eval_results[\u001b[33m\"\u001b[39m\u001b[33mresults\u001b[39m\u001b[33m\"\u001b[39m].extend(\n\u001b[32m   1592\u001b[39m         \u001b[38;5;28mself\u001b[39m.client._select_eval_results(evaluator_response)\n\u001b[32m   1593\u001b[39m     )\n\u001b[32m   1594\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._upload_results:\n\u001b[32m   1595\u001b[39m         \u001b[38;5;66;03m# TODO: This is a hack\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py:353\u001b[39m, in \u001b[36mDynamicRunEvaluator.evaluate_run\u001b[39m\u001b[34m(self, run, example, evaluator_run_id)\u001b[39m\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(run, \u001b[33m\"\u001b[39m\u001b[33msession_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    352\u001b[39m     metadata[\u001b[33m\"\u001b[39m\u001b[33mexperiment\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mstr\u001b[39m(run.session_id)\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlangsmith_extra\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluator_run_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_result(result, evaluator_run_id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py:779\u001b[39m, in \u001b[36m_normalize_evaluator_func.<locals>.wrapper\u001b[39m\u001b[34m(run, example)\u001b[39m\n\u001b[32m    777\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(run: Run, example: Optional[Example]) -> _RUNNABLE_OUTPUT:\n\u001b[32m    778\u001b[39m     (args, kwargs, _) = _prepare_inputs(run, example)\n\u001b[32m--> \u001b[39m\u001b[32m779\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/llm.py:566\u001b[39m, in \u001b[36mcreate_llm_as_judge.<locals>._wrapped_evaluator\u001b[39m\u001b[34m(inputs, outputs, reference_outputs, **kwargs)\u001b[39m\n\u001b[32m    554\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrapped_evaluator\u001b[39m(\n\u001b[32m    555\u001b[39m     *,\n\u001b[32m    556\u001b[39m     inputs: Optional[Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    559\u001b[39m     **kwargs,\n\u001b[32m    560\u001b[39m ) -> Union[EvaluatorResult, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[32m    561\u001b[39m     run_name = (\n\u001b[32m    562\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mllm_as_judge\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    563\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m feedback_key == \u001b[33m\"\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    564\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mllm_as_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeedback_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_judge\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    565\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m     res = \u001b[43m_run_evaluator_untyped\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeedback_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeedback_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreference_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreference_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_raw_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_schema\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    574\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mStructuredPrompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    577\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m output_schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    578\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m res  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/utils.py:219\u001b[39m, in \u001b[36m_run_evaluator_untyped\u001b[39m\u001b[34m(run_name, scorer, feedback_key, return_raw_outputs, ls_framework, **kwargs)\u001b[39m\n\u001b[32m    213\u001b[39m                 t.log_feedback(\n\u001b[32m    214\u001b[39m                     key=results[\u001b[33m\"\u001b[39m\u001b[33mkey\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    215\u001b[39m                     score=results[\u001b[33m\"\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    216\u001b[39m                     comment=results[\u001b[33m\"\u001b[39m\u001b[33mcomment\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    217\u001b[39m                 )\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     results = \u001b[43m_run_scorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m     _add_metadata_and_inputs_to_run_tree(run_name, ls_framework, results)\n\u001b[32m    222\u001b[39m \u001b[38;5;66;03m# Return single result or list of results\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/utils.py:132\u001b[39m, in \u001b[36m_run_evaluator_untyped.<locals>._run_scorer\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;129m@traceable\u001b[39m(name=run_name)\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_scorer\u001b[39m(**kwargs: Any):\n\u001b[32m    131\u001b[39m     \u001b[38;5;66;03m# Get the initial score\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m     score = \u001b[43mscorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m return_raw_outputs:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m score\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/openevals/llm.py:238\u001b[39m, in \u001b[36m_create_llm_as_judge_scorer.<locals>.get_score\u001b[39m\u001b[34m(inputs, outputs, reference_outputs, **kwargs)\u001b[39m\n\u001b[32m    236\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    237\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33ma `model` string is required (e.g. \u001b[39m\u001b[33m'\u001b[39m\u001b[33mopenai:o3-mini\u001b[39m\u001b[33m'\u001b[39m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     judge = \u001b[43minit_chat_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(judge, BaseChatModel):\n\u001b[32m    241\u001b[39m     judge_with_structured_output = judge.with_structured_output(\n\u001b[32m    242\u001b[39m         schema\n\u001b[32m    243\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    248\u001b[39m         }\n\u001b[32m    249\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain/chat_models/base.py:316\u001b[39m, in \u001b[36minit_chat_model\u001b[39m\u001b[34m(model, model_provider, configurable_fields, config_prefix, **kwargs)\u001b[39m\n\u001b[32m    308\u001b[39m     warnings.warn(\n\u001b[32m    309\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_prefix\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m has been set but no fields are configurable. Set \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    310\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`configurable_fields=(...)` to specify the model params that are \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    311\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mconfigurable.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    312\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    313\u001b[39m     )\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m configurable_fields:\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_init_chat_model_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model:\n\u001b[32m    322\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m] = model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain/chat_models/base.py:368\u001b[39m, in \u001b[36m_init_chat_model_helper\u001b[39m\u001b[34m(model, model_provider, **kwargs)\u001b[39m\n\u001b[32m    365\u001b[39m     _check_pkg(\u001b[33m\"\u001b[39m\u001b[33mlangchain_google_vertexai\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    366\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_google_vertexai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatVertexAI\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mChatVertexAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_provider == \u001b[33m\"\u001b[39m\u001b[33mgoogle_genai\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    370\u001b[39m     _check_pkg(\u001b[33m\"\u001b[39m\u001b[33mlangchain_google_genai\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:239\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    237\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    238\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_google_vertexai/chat_models.py:1831\u001b[39m, in \u001b[36mChatVertexAI.__init__\u001b[39m\u001b[34m(self, model_name, **kwargs)\u001b[39m\n\u001b[32m   1825\u001b[39m         suggestion = (\n\u001b[32m   1826\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Did you mean: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestions[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m?\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m suggestions \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1827\u001b[39m         )\n\u001b[32m   1828\u001b[39m         logger.warning(\n\u001b[32m   1829\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected argument \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m provided to ChatVertexAI.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1830\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1831\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_core/load/serializable.py:116\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    115\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419  # Intentional blank docstring\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/langchain_google_vertexai/_base.py:156\u001b[39m, in \u001b[36m_VertexAIBase.validate_project\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    154\u001b[39m         \u001b[38;5;28mself\u001b[39m.project = \u001b[38;5;28mself\u001b[39m.credentials.project_id\n\u001b[32m    155\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m         \u001b[38;5;28mself\u001b[39m.project = \u001b[43minitializer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mglobal_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproject\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/initializer.py:367\u001b[39m, in \u001b[36m_Config.project\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    357\u001b[39m project_not_found_exception_str = (\n\u001b[32m    358\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mUnable to find your project. Please provide a project ID by:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    359\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m- Passing a constructor argument\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    363\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m- To create a Google Cloud project, please follow guidance at https://developers.google.com/workspace/guides/create-project\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    364\u001b[39m )\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_set_project_as_env_var_or_google_auth_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    368\u001b[39m     project_id = \u001b[38;5;28mself\u001b[39m._project\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m GoogleAuthError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/initializer.py:114\u001b[39m, in \u001b[36m_Config._set_project_as_env_var_or_google_auth_default\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    112\u001b[39m         \u001b[38;5;28mself\u001b[39m._project = project_number\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     credentials, project = \u001b[43mgoogle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28mself\u001b[39m._credentials = \u001b[38;5;28mself\u001b[39m._credentials \u001b[38;5;129;01mor\u001b[39;00m credentials\n\u001b[32m    116\u001b[39m     \u001b[38;5;28mself\u001b[39m._project = project\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/auth/_default.py:705\u001b[39m, in \u001b[36mdefault\u001b[39m\u001b[34m(scopes, request, quota_project_id, default_scopes)\u001b[39m\n\u001b[32m    693\u001b[39m checkers = (\n\u001b[32m    694\u001b[39m     \u001b[38;5;66;03m# Avoid passing scopes here to prevent passing scopes to user credentials.\u001b[39;00m\n\u001b[32m    695\u001b[39m     \u001b[38;5;66;03m# with_scopes_if_required() below will ensure scopes/default scopes are\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    701\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m: _get_gce_credentials(request, quota_project_id=quota_project_id),\n\u001b[32m    702\u001b[39m )\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m checker \u001b[38;5;129;01min\u001b[39;00m checkers:\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m     credentials, project_id = \u001b[43mchecker\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m credentials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    707\u001b[39m         credentials = with_scopes_if_required(\n\u001b[32m    708\u001b[39m             credentials, scopes, default_scopes=default_scopes\n\u001b[32m    709\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/auth/_default.py:701\u001b[39m, in \u001b[36mdefault.<locals>.<lambda>\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    687\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauth\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcredentials\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CredentialsWithQuotaProject\n\u001b[32m    689\u001b[39m explicit_project_id = os.environ.get(\n\u001b[32m    690\u001b[39m     environment_vars.PROJECT, os.environ.get(environment_vars.LEGACY_PROJECT)\n\u001b[32m    691\u001b[39m )\n\u001b[32m    693\u001b[39m checkers = (\n\u001b[32m    694\u001b[39m     \u001b[38;5;66;03m# Avoid passing scopes here to prevent passing scopes to user credentials.\u001b[39;00m\n\u001b[32m    695\u001b[39m     \u001b[38;5;66;03m# with_scopes_if_required() below will ensure scopes/default scopes are\u001b[39;00m\n\u001b[32m    696\u001b[39m     \u001b[38;5;66;03m# safely set on the returned credentials since requires_scopes will\u001b[39;00m\n\u001b[32m    697\u001b[39m     \u001b[38;5;66;03m# guard against setting scopes on user credentials.\u001b[39;00m\n\u001b[32m    698\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m: _get_explicit_environ_credentials(quota_project_id=quota_project_id),\n\u001b[32m    699\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m: _get_gcloud_sdk_credentials(quota_project_id=quota_project_id),\n\u001b[32m    700\u001b[39m     _get_gae_credentials,\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43m_get_gce_credentials\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    702\u001b[39m )\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m checker \u001b[38;5;129;01min\u001b[39;00m checkers:\n\u001b[32m    705\u001b[39m     credentials, project_id = checker()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/auth/_default.py:404\u001b[39m, in \u001b[36m_get_gce_credentials\u001b[39m\u001b[34m(request, quota_project_id)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m request \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    402\u001b[39m     request = google.auth.transport._http_client.Request()\n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m_metadata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_on_gce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    405\u001b[39m     \u001b[38;5;66;03m# Get the project ID.\u001b[39;00m\n\u001b[32m    406\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    407\u001b[39m         project_id = _metadata.get_project_id(request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/auth/compute_engine/_metadata.py:119\u001b[39m, in \u001b[36mis_on_gce\u001b[39m\u001b[34m(request)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mis_on_gce\u001b[39m(request):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Checks to see if the code runs on Google Compute Engine\u001b[39;00m\n\u001b[32m    111\u001b[39m \n\u001b[32m    112\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m        bool: True if the code runs on Google Compute Engine, False otherwise.\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    122\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m os.name == \u001b[33m\"\u001b[39m\u001b[33mnt\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    123\u001b[39m         \u001b[38;5;66;03m# TODO: implement GCE residency detection on Windows\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/auth/compute_engine/_metadata.py:201\u001b[39m, in \u001b[36mping\u001b[39m\u001b[34m(request, timeout, retry_count)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m backoff:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         response = \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_get_metadata_ip_root\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_mtls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m         metadata_flavor = response.headers.get(_METADATA_FLAVOR_HEADER)\n\u001b[32m    209\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m    210\u001b[39m             response.status == http_client.OK\n\u001b[32m    211\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m metadata_flavor == _METADATA_FLAVOR_VALUE\n\u001b[32m    212\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AI/AyushDataScience/9_GenAI/llmops/end_to_end_rag_project/.venv/lib/python3.12/site-packages/google/auth/transport/_http_client.py:105\u001b[39m, in \u001b[36mRequest.__call__\u001b[39m\u001b[34m(self, url, method, body, headers, timeout, **kwargs)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    104\u001b[39m     _helpers.request_log(_LOGGER, method, url, body, headers)\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m     response = connection.getresponse()\n\u001b[32m    107\u001b[39m     _helpers.response_log(_LOGGER, response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1336\u001b[39m, in \u001b[36mHTTPConnection.request\u001b[39m\u001b[34m(self, method, url, body, headers, encode_chunked)\u001b[39m\n\u001b[32m   1333\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, url, body=\u001b[38;5;28;01mNone\u001b[39;00m, headers={}, *,\n\u001b[32m   1334\u001b[39m             encode_chunked=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1335\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1382\u001b[39m, in \u001b[36mHTTPConnection._send_request\u001b[39m\u001b[34m(self, method, url, body, headers, encode_chunked)\u001b[39m\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(body, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   1379\u001b[39m     \u001b[38;5;66;03m# RFC 2616 Section 3.7.1 says that text default has a\u001b[39;00m\n\u001b[32m   1380\u001b[39m     \u001b[38;5;66;03m# default charset of iso-8859-1.\u001b[39;00m\n\u001b[32m   1381\u001b[39m     body = _encode(body, \u001b[33m'\u001b[39m\u001b[33mbody\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1382\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1331\u001b[39m, in \u001b[36mHTTPConnection.endheaders\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[32m-> \u001b[39m\u001b[32m1331\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1091\u001b[39m, in \u001b[36mHTTPConnection._send_output\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1089\u001b[39m msg = \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mself\u001b[39m._buffer)\n\u001b[32m   1090\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffer[:]\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1094\u001b[39m \n\u001b[32m   1095\u001b[39m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n\u001b[32m   1096\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(message_body, \u001b[33m'\u001b[39m\u001b[33mread\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m   1097\u001b[39m         \u001b[38;5;66;03m# Let file-like take precedence over byte-like.  This\u001b[39;00m\n\u001b[32m   1098\u001b[39m         \u001b[38;5;66;03m# is needed to allow the current position of mmap'ed\u001b[39;00m\n\u001b[32m   1099\u001b[39m         \u001b[38;5;66;03m# files to be taken into account.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1035\u001b[39m, in \u001b[36mHTTPConnection.send\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1033\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.sock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1034\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_open:\n\u001b[32m-> \u001b[39m\u001b[32m1035\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1036\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1037\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m NotConnected()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1001\u001b[39m, in \u001b[36mHTTPConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    999\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Connect to the host and port specified in __init__.\"\"\"\u001b[39;00m\n\u001b[32m   1000\u001b[39m sys.audit(\u001b[33m\"\u001b[39m\u001b[33mhttp.client.connect\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m.host, \u001b[38;5;28mself\u001b[39m.port)\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m \u001b[38;5;28mself\u001b[39m.sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[38;5;66;03m# Might fail in OSs that don't implement TCP_NODELAY\u001b[39;00m\n\u001b[32m   1004\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/socket.py:837\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, all_errors)\u001b[39m\n\u001b[32m    835\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[32m    836\u001b[39m     sock.bind(source_address)\n\u001b[32m--> \u001b[39m\u001b[32m837\u001b[39m \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[32m    839\u001b[39m exceptions.clear()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "from openevals.llm import create_llm_as_judge\n",
    "from openevals.prompts import CONCISENESS_PROMPT  # better than conciseness for RAG\n",
    "\n",
    "client = Client()\n",
    "\n",
    "qa_evaluator = create_llm_as_judge(\n",
    "    prompt=CONCISENESS_PROMPT,\n",
    "    feedback_key=\"qa_correctness\",\n",
    "    model=\"gemini:model/gemini-2.5-flash-lite\",\n",
    ")\n",
    "\n",
    "experiment_results = client.evaluate(\n",
    "    answer_ai_report_question,   # ← target MUST be positional\n",
    "    \"AgenticAImemoryqa\",          # ← data MUST be positional\n",
    "    evaluators=[qa_evaluator],\n",
    "    experiment_prefix=\"test-agenticAIReport-qa-rag\",\n",
    "    metadata={\n",
    "        \"variant\": \"RAG with FAISS + agent memory\",\n",
    "        \"chunk_size\": 1000,\n",
    "        \"chunk_overlap\": 200,\n",
    "        \"k\": 5,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e9f39a",
   "metadata": {},
   "source": [
    "## Custom Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658fc7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.schemas import Run, Example\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def correctness_evaluator(run: Run, example: Example) -> dict:\n",
    "    \"\"\"\n",
    "    Custom LLM-as-a-Judge evaluator for correctness.\n",
    "    \n",
    "    Correctness means how well the actual model output matches the reference output \n",
    "    in terms of factual accuracy, coverage, and meaning.\n",
    "    \n",
    "    Args:\n",
    "        run: The Run object containing the actual outputs\n",
    "        example: The Example object containing the expected outputs\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'score' (1 for correct, 0 for incorrect) and 'reasoning'\n",
    "    \"\"\"\n",
    "    # Extract actual and expected outputs\n",
    "    actual_output = run.outputs.get(\"answer\", \"\")\n",
    "    expected_output = example.outputs.get(\"answer\", \"\")\n",
    "    input_question = example.inputs.get(\"question\", \"\")\n",
    "    \n",
    "    # Define the evaluation prompt\n",
    "    eval_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are an evaluator whose job is to judge correctness.\n",
    "\n",
    "        Correctness means how well the actual model output matches the reference output in terms of factual accuracy, coverage, and meaning.\n",
    "\n",
    "        - If the actual output matches the reference output semantically (even if wording differs), it should be marked correct.\n",
    "        - If the output misses key facts, introduces contradictions, or is factually incorrect, it should be marked incorrect.\n",
    "\n",
    "        Do not penalize for stylistic or formatting differences unless they change meaning.\"\"\"),\n",
    "                (\"human\", \"\"\"<example>\n",
    "        <input>\n",
    "        {input}\n",
    "        </input>\n",
    "\n",
    "        <output>\n",
    "        Expected Output: {expected_output}\n",
    "\n",
    "        Actual Output: {actual_output}\n",
    "        </output>\n",
    "        </example>\n",
    "\n",
    "        Please grade the following agent run given the input, expected output, and actual output.\n",
    "        Focus only on correctness (semantic and factual alignment).\n",
    "\n",
    "        Respond with:\n",
    "        1. A brief reasoning (1-2 sentences)\n",
    "        2. A final verdict: either \"CORRECT\" or \"INCORRECT\"\n",
    "\n",
    "        Format your response as:\n",
    "        Reasoning: [your reasoning]\n",
    "        Verdict: [CORRECT or INCORRECT]\"\"\")\n",
    "    ])\n",
    "    \n",
    "    # Initialize LLM (using Gemini as shown in your config)\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.5-pro\",\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Create chain and invoke\n",
    "    chain = eval_prompt | llm\n",
    "    \n",
    "    try:\n",
    "        response = chain.invoke({\n",
    "            \"input\": input_question,\n",
    "            \"expected_output\": expected_output,\n",
    "            \"actual_output\": actual_output\n",
    "        })\n",
    "        \n",
    "        response_text = response.content\n",
    "        \n",
    "        # Parse the response\n",
    "        reasoning = \"\"\n",
    "        verdict = \"\"\n",
    "        \n",
    "        for line in response_text.split('\\n'):\n",
    "            if line.startswith(\"Reasoning:\"):\n",
    "                reasoning = line.replace(\"Reasoning:\", \"\").strip()\n",
    "            elif line.startswith(\"Verdict:\"):\n",
    "                verdict = line.replace(\"Verdict:\", \"\").strip()\n",
    "        \n",
    "        # Convert verdict to score (1 for correct, 0 for incorrect)\n",
    "        score = 1 if \"CORRECT\" in verdict.upper() else 0\n",
    "        \n",
    "        return {\n",
    "            \"key\": \"correctness\",\n",
    "            \"score\": score,\n",
    "            \"reasoning\": reasoning,\n",
    "            \"comment\": f\"Verdict: {verdict}\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"key\": \"correctness\",\n",
    "            \"score\": 0,\n",
    "            \"reasoning\": f\"Error during evaluation: {str(e)}\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f618bb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation with the custom correctness evaluator\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "# Define evaluators - using custom correctness evaluator\n",
    "evaluators = [correctness_evaluator]\n",
    "\n",
    "dataset_name = \"AgenticAIReportGoldens\"\n",
    "\n",
    "# Run evaluation\n",
    "experiment_results = evaluate(\n",
    "    answer_ai_report_question,\n",
    "    data=dataset_name,\n",
    "    evaluators=evaluators,\n",
    "    experiment_prefix=\"agenticAIReport-correctness-eval\",\n",
    "    description=\"Evaluating RAG system with custom correctness evaluator (LLM-as-a-Judge)\",\n",
    "    metadata={\n",
    "        \"variant\": \"RAG with FAISS and AI Engineering Report\",\n",
    "        \"evaluator\": \"custom_correctness_llm_judge\",\n",
    "        \"model\": \"gemini-2.5-pro\",\n",
    "        \"chunk_size\": 1000,\n",
    "        \"chunk_overlap\": 200,\n",
    "        \"k\": 5,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluation completed! Check the LangSmith UI for detailed results.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652eece0",
   "metadata": {},
   "source": [
    "## Combo of both "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48fc2b6",
   "metadata": {},
   "source": [
    "# Example: Combine custom correctness evaluator with LangChain's built-in evaluators\n",
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "# Combine custom and built-in evaluators\n",
    "combined_evaluators = [\n",
    "    correctness_evaluator,  # Custom LLM-as-a-Judge\n",
    "   qa_evaluator,  # built in evaluator\n",
    "]\n",
    "\n",
    "# Run evaluation with multiple evaluators\n",
    "# Uncomment to run:\n",
    "# experiment_results_combined = evaluate(\n",
    "#     answer_ai_report_question,\n",
    "#     data=dataset_name,\n",
    "#     evaluators=combined_evaluators,\n",
    "#     experiment_prefix=\"agenticAIReport-multi-eval\",\n",
    "#     description=\"Evaluating RAG system with multiple evaluators\",\n",
    "#     metadata={\n",
    "#         \"variant\": \"RAG with FAISS\",\n",
    "#         \"evaluators\": \"correctness + cot_qa\",\n",
    "#         \"chunk_size\": 1000,\n",
    "#         \"chunk_overlap\": 200,\n",
    "#         \"k\": 5,\n",
    "#     },\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
