{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arDizHpqFDk-"
      },
      "outputs": [],
      "source": [
        "import langchain\n",
        "import google.genai\n",
        "from dotenv import load_dotenv\n",
        "!pip install langchain_google_genai -q\n",
        "!pip install -U langchain langchain-core langchain-community -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf -q\n",
        "!pip install -U langchain-text-splitters -q"
      ],
      "metadata": {
        "id": "OAqbP-aVPa9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
        "\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash-lite\",\n",
        "    temperature=1.0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z8PfVANFJue",
        "outputId": "267750c3-da99-4ecf-a51b-d093188e083c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Google AI API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.invoke('hello').content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7kQ1-GRaGkpr",
        "outputId": "c1980651-4af5-4d62-f4a6-364c55ff47a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hi there! How can I help you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Ingestion"
      ],
      "metadata": {
        "id": "JoBY01IqOUrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data load\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "file_path = \"/content/Entrepreneurial Intention Prediction Report.pdf\"\n",
        "loader = PyMuPDFLoader(file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfLksCouGs8s",
        "outputId": "3715314c-aa06-45d7-8726-d509078e04de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs = loader.load()\n",
        "document = [docs[i].page_content for i in range(len(docs))][0]"
      ],
      "metadata": {
        "id": "VKtZZ5mqPGwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#text splitting\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
        "texts = text_splitter.split_text(document)"
      ],
      "metadata": {
        "id": "SNPasKKfPYEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLrS0ywwQEAU",
        "outputId": "3f17613e-5bbe-4fce-fa7a-509473c721f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Entrepreneurial Intention Prediction \\nReport \\nName: Ayush Vishwakarma\\u200b',\n",
              " 'Dataset: GEM APS Global Individual Level Data\\u200b',\n",
              " 'Target Variable: Entrepreneurial Intention (futsupno)\\u200b\\nDate: 07 October 2025 \\n \\n1. Introduction',\n",
              " 'The objective of this analysis is to model and predict entrepreneurial intention among individuals',\n",
              " 'based on demographic, attitudinal, and social network features. The GEM 2020 Adult',\n",
              " 'Population Survey (APS) dataset provides various attributes related to personal characteristics,',\n",
              " 'motivations, and perceptions regarding entrepreneurship.',\n",
              " 'The target variable, futsupno, indicates whether an individual intends to engage in',\n",
              " 'entrepreneurial activities in the near future. A value of 1 represents intention to start a',\n",
              " 'business,',\n",
              " 'while 0 indicates no intention. \\n \\n2. Selected Features',\n",
              " 'Seventeen predictors were used for modeling:',\n",
              " '●\\u200b Demographic: age, gender, hhsize, gemhhinc, gemeduc\\u200b',\n",
              " '●\\u200b Entrepreneurial awareness and skills: knowentr, opportl, suskilll, \\nfearfaill, easystartl\\u200b',\n",
              " '●\\u200b Personal attitudes: proact, creativ, vision\\u200b',\n",
              " '●\\u200b Social network influence: nbgoodcl, nbstatusl, nbmedial, nbsocentl\\u200b']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#embedding\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
        "vector = embeddings.embed_documents(texts)\n",
        "len(vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmMXjMG4Rtc2",
        "outputId": "a51121fe-7014-4b40-b8b1-9e5ef63e02a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#vector store\n",
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "\n",
        "\n",
        "vectorstore = InMemoryVectorStore.from_texts(\n",
        "    texts,\n",
        "    embedding=embeddings,\n",
        ")\n",
        "\n",
        "# Use the vectorstore as a retriever\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# # Retrieve the most similar text\n",
        "# retrieved_documents = retriever.invoke(\"what is Dataset\")\n",
        "\n",
        "# # show the retrieved document's content\n",
        "# retrieved_documents[0].page_content\n",
        "query = \"what is Dataset\"\n",
        "docs = vectorstore.similarity_search(query, k=4)\n",
        "\n",
        "# Display the results\n",
        "for i, doc in enumerate(docs):\n",
        "    print(f\"Document {i+1}:\")\n",
        "    print(doc.page_content)\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMl5BevgSuFY",
        "outputId": "acee8d0a-e103-4f5c-b72f-921f42bd0faf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1:\n",
            "Population Survey (APS) dataset provides various attributes related to personal characteristics,\n",
            "--------------------------------------------------\n",
            "Document 2:\n",
            "Dataset: GEM APS Global Individual Level Data​\n",
            "--------------------------------------------------\n",
            "Document 3:\n",
            "while 0 indicates no intention. \n",
            " \n",
            "2. Selected Features\n",
            "--------------------------------------------------\n",
            "Document 4:\n",
            "●​ Demographic: age, gender, hhsize, gemhhinc, gemeduc​\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template=\"\"\"You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "Use ten sentences maximum and keep the answer concise.\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "Answer:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "1ezRJ_jxTaVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=ChatPromptTemplate.from_template(template)\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KasB9E4XVSWn",
        "outputId": "65f3b1f9-56a2-4d3d-c93f-fe10a1d40884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks.\\nUse the following pieces of retrieved context to answer the question.\\nIf you don't know the answer, just say that you don't know.\\nUse ten sentences maximum and keep the answer concise.\\nQuestion: {question}\\nContext: {context}\\nAnswer:\\n\"), additional_kwargs={})])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever,  \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        ")\n",
        "rag_chain.invoke(\"dataset\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krY4LJWaVdhU",
        "outputId": "b0b104cc-791d-4698-8235-524058b8184e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='The GEM APS Global Individual Level Data dataset offers a variety of personal attributes. This dataset is also known as the Population Survey (APS) dataset. It includes information on personal characteristics. The dataset has seventeen predictors that were used for modeling.', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019b4c64-19de-71e3-951b-12027123fd10-0', usage_metadata={'input_tokens': 298, 'output_tokens': 47, 'total_tokens': 345, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0c2uInO_Yrjr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}